{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec0b26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiinir/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from functools import partial, wraps\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ToTensor, Pad, Resize, ToPILImage, InterpolationMode, Normalize\n",
    "\n",
    "from dalle2_laion import ModelLoadConfig, DalleModelManager\n",
    "from dalle2_laion.scripts import InferenceScript\n",
    "from dalle2_pytorch.vqgan_vae import NullVQGanVAE, VQGanVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5e71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, im_dir, caption_dir):\n",
    "        self.im_dir = im_dir\n",
    "        self.caption_dir = caption_dir\n",
    "        self.im_fnames = sorted(os.listdir(im_dir))\n",
    "        self.caption_fnames = sorted(os.listdir(caption_dir))\n",
    "        self.to_tensor_transform = ToTensor()\n",
    "        self.pad_transform = lambda im, pad_right, pad_bottom: Pad(padding=(0, 0, pad_right, pad_bottom))(im)\n",
    "        self.resize_transform = Resize((256, 256), interpolation=InterpolationMode.BILINEAR)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = Image.open(os.path.join(self.im_dir, self.im_fnames[idx]))\n",
    "        w, h = im.size\n",
    "        im = self.to_tensor_transform(im)\n",
    "        new_size = max(w, h)\n",
    "        im = self.pad_transform(im, new_size - w, new_size - h)\n",
    "        im = self.resize_transform(im)\n",
    "\n",
    "        with open(os.path.join(self.caption_dir, self.caption_fnames[idx]), 'r') as captions_f:\n",
    "            captions = [caption.strip() for caption in captions_f.readlines()]\n",
    "\n",
    "        return self.im_fnames[idx], im, captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_fnames)\n",
    "\n",
    "        \n",
    "class ExampleInference(InferenceScript):\n",
    "    def run(self, text: str):\n",
    "        \"\"\"\n",
    "        Takes a string and returns a single image.\n",
    "        \"\"\"\n",
    "        text = [text]\n",
    "        image_embedding_map = self._sample_prior(text)\n",
    "        image_embedding = image_embedding_map[0][0].unsqueeze(0)\n",
    "        image_map = self._sample_decoder(text=text, image_embed=image_embedding)\n",
    "        return image_map[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8a62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def first(arr, d = None):\n",
    "    if len(arr) == 0:\n",
    "        return d\n",
    "    return arr[0]\n",
    "\n",
    "def maybe(fn):\n",
    "    @wraps(fn)\n",
    "    def inner(x, *args, **kwargs):\n",
    "        if not exists(x):\n",
    "            return x\n",
    "        return fn(x, *args, **kwargs)\n",
    "    return inner\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def cast_tuple(val, length = None, validate = True):\n",
    "    if isinstance(val, list):\n",
    "        val = tuple(val)\n",
    "\n",
    "    out = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n",
    "\n",
    "    if exists(length) and validate:\n",
    "        assert len(out) == length\n",
    "\n",
    "    return out\n",
    "\n",
    "@contextmanager\n",
    "def null_context(*args, **kwargs):\n",
    "    yield\n",
    "\n",
    "def eval_decorator(fn):\n",
    "    def inner(model, *args, **kwargs):\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "        out = fn(model, *args, **kwargs)\n",
    "        model.train(was_training)\n",
    "        return out\n",
    "    return inner\n",
    "\n",
    "def resize_image_to(\n",
    "    image,\n",
    "    target_image_size,\n",
    "    clamp_range = None,\n",
    "    nearest = False,\n",
    "    **kwargs\n",
    "):\n",
    "    orig_image_size = image.shape[-1]\n",
    "\n",
    "    if orig_image_size == target_image_size:\n",
    "        return image\n",
    "\n",
    "    if not nearest:\n",
    "        scale_factors = target_image_size / orig_image_size\n",
    "        out = resize(image, scale_factors = scale_factors, **kwargs)\n",
    "    else:\n",
    "        out = F.interpolate(image, target_image_size, mode = 'bicubic') #'nearest')\n",
    "\n",
    "    if exists(clamp_range):\n",
    "        out = out.clamp(*clamp_range)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e983c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def module_device(module):\n",
    "    if isinstance(module, nn.Identity):\n",
    "        return 'cpu' # It doesn't matter\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "@contextmanager\n",
    "def one_unet_in_gpu(self, unet_number = None, unet = None, device=torch.device('cpu')):\n",
    "    assert exists(unet_number) ^ exists(unet)\n",
    "\n",
    "    if exists(unet_number):\n",
    "        unet = self.get_unet(unet_number)\n",
    "\n",
    "    self.to(device)\n",
    "\n",
    "    devices = [module_device(unet) for unet in self.unets]\n",
    "    self.unets.cpu()\n",
    "    unet.to(device)\n",
    "\n",
    "    yield\n",
    "    \n",
    "    for unet, device in zip(self.unets, devices):\n",
    "        unet.to(device)\n",
    "\n",
    "\n",
    "def p_mean_variance_custom(self, unet, x, t, image_embed, noise_scheduler, text_encodings = None, lowres_cond_img = None, clip_denoised = True, predict_x_start = False, learned_variance = False, cond_scale = 1., model_output = None, lowres_noise_level = None):\n",
    "    assert not (cond_scale != 1. and not self.can_classifier_guidance), 'the decoder was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n",
    "\n",
    "    pred = default(model_output, lambda: unet.forward_with_cond_scale(x, t, image_embed = image_embed, text_encodings = text_encodings, cond_scale = cond_scale, lowres_cond_img = lowres_cond_img, lowres_noise_level = lowres_noise_level))\n",
    "\n",
    "    if learned_variance:\n",
    "        pred, var_interp_frac_unnormalized = pred.chunk(2, dim = 1)\n",
    "\n",
    "    if predict_x_start:\n",
    "        x_recon = pred\n",
    "    else:\n",
    "        x_recon = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n",
    "\n",
    "    if clip_denoised:\n",
    "        x_recon = self.dynamic_threshold(x_recon)\n",
    "\n",
    "    model_mean, posterior_variance, posterior_log_variance = noise_scheduler.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
    "\n",
    "    if learned_variance:\n",
    "        # if learned variance, posterio variance and posterior log variance are predicted by the network\n",
    "        # by an interpolation of the max and min log beta values\n",
    "        # eq 15 - https://arxiv.org/abs/2102.09672\n",
    "        min_log = extract(noise_scheduler.posterior_log_variance_clipped, t, x.shape)\n",
    "        max_log = extract(torch.log(noise_scheduler.betas), t, x.shape)\n",
    "        var_interp_frac = unnormalize_zero_to_one(var_interp_frac_unnormalized)\n",
    "\n",
    "        if self.learned_variance_constrain_frac:\n",
    "            var_interp_frac = var_interp_frac.sigmoid()\n",
    "\n",
    "        posterior_log_variance = var_interp_frac * max_log + (1 - var_interp_frac) * min_log\n",
    "        posterior_variance = posterior_log_variance.exp()\n",
    "\n",
    "    return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "\n",
    "def p_sample_custom(self, unet, x, t, image_embed, noise_scheduler, text_encodings = None, cond_scale = 1., lowres_cond_img = None, predict_x_start = False, learned_variance = False, clip_denoised = True, lowres_noise_level = None):\n",
    "    b, *_, device = *x.shape, x.device\n",
    "    model_mean, _, model_log_variance = p_mean_variance_custom(self, unet, x = x, t = t, image_embed = image_embed, text_encodings = text_encodings, cond_scale = cond_scale, lowres_cond_img = lowres_cond_img, clip_denoised = clip_denoised, predict_x_start = predict_x_start, noise_scheduler = noise_scheduler, learned_variance = learned_variance, lowres_noise_level = lowres_noise_level)\n",
    "    noise = torch.randn_like(x)\n",
    "    # no noise when t == 0\n",
    "    nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "    return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "\n",
    "def p_sample_loop_ddpm_custom(\n",
    "    self,\n",
    "    unet,\n",
    "    shape,\n",
    "    img_start,\n",
    "    image_embed,\n",
    "    noise_scheduler,\n",
    "    predict_x_start = False,\n",
    "    learned_variance = False,\n",
    "    clip_denoised = True,\n",
    "    lowres_cond_img = None,\n",
    "    text_encodings = None,\n",
    "    cond_scale = 1,\n",
    "    is_latent_diffusion = False,\n",
    "    lowres_noise_level = None,\n",
    "    inpaint_image = None,\n",
    "    inpaint_mask = None,\n",
    "    inpaint_resample_times = 5,\n",
    "    gt=None\n",
    "):\n",
    "    for param in unet.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    device = self.device\n",
    "    \n",
    "    b = shape[0]\n",
    "    img = img_start.to(device) #torch.randn(shape, device = device)\n",
    "\n",
    "    resample_times = 1\n",
    "\n",
    "    if not is_latent_diffusion:\n",
    "        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n",
    "    \n",
    "    img = F.interpolate(img, size=(256, 256), mode='bicubic')\n",
    "\n",
    "    text_encodings.requires_grad = True\n",
    "    image_embed.requires_grad = True\n",
    "    lowres_cond_img.requires_grad = True\n",
    "    img.requires_grad = True\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW([lowres_cond_img], lr=1e-4)\n",
    "    \n",
    "    psnrs = []\n",
    "    \n",
    "    for time in tqdm(reversed(range(0, noise_scheduler.num_timesteps)), desc = 'sampling loop time step', total = noise_scheduler.num_timesteps):\n",
    "        is_last_timestep = time == 0\n",
    "\n",
    "        \n",
    "        if time > 100:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        for r in reversed(range(0, resample_times)):\n",
    "            is_last_resample_step = r == 0\n",
    "\n",
    "            times = torch.full((b,), time, device = device, dtype = torch.long)\n",
    "\n",
    "            img = p_sample_custom(\n",
    "                self,\n",
    "                unet,\n",
    "                img.detach(),\n",
    "                times,\n",
    "                image_embed = image_embed,\n",
    "                text_encodings = text_encodings,\n",
    "                cond_scale = cond_scale,\n",
    "                lowres_cond_img = lowres_cond_img,\n",
    "                lowres_noise_level = lowres_noise_level,\n",
    "                predict_x_start = predict_x_start,\n",
    "                noise_scheduler = noise_scheduler,\n",
    "                learned_variance = learned_variance,\n",
    "                clip_denoised = clip_denoised\n",
    "            )\n",
    "            loss = loss_fn(img, gt.unsqueeze(0))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            psnrs.append(psnr(self.unnormalize_img(img.detach()).squeeze(0), gt.cpu()).item())\n",
    "\n",
    "    unnormalize_img = self.unnormalize_img(img)\n",
    "    return unnormalize_img, psnrs\n",
    "\n",
    "\n",
    "def p_sample_loop_custom(self, *args, noise_scheduler, timesteps = None, **kwargs):\n",
    "    num_timesteps = noise_scheduler.num_timesteps\n",
    "\n",
    "    timesteps = default(timesteps, num_timesteps)\n",
    "    assert timesteps <= num_timesteps\n",
    "    is_ddim = timesteps < num_timesteps\n",
    "\n",
    "    return p_sample_loop_ddpm_custom(inference.model_manager.decoder_info.model,*args, noise_scheduler = noise_scheduler, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def sample_custom(\n",
    "    self,\n",
    "    lowres_cond_img=None,\n",
    "    image = None,\n",
    "    image_embed = None,\n",
    "    text = None,\n",
    "    text_encodings = None,\n",
    "    batch_size = 1,\n",
    "    cond_scale = 1.,\n",
    "    start_at_unet_number = 1,\n",
    "    stop_at_unet_number = None,\n",
    "    distributed = False,\n",
    "    inpaint_image = None,\n",
    "    inpaint_mask = None,\n",
    "    inpaint_resample_times = 5,\n",
    "    gt=None\n",
    "):\n",
    "    assert self.unconditional or exists(image_embed), 'image embed must be present on sampling from decoder unless if trained unconditionally'\n",
    "\n",
    "    if not self.unconditional:\n",
    "        batch_size = image_embed.shape[0]\n",
    "\n",
    "    if exists(text) and not exists(text_encodings) and not self.unconditional:\n",
    "        assert exists(self.clip)\n",
    "        _, text_encodings = self.clip.embed_text(text)\n",
    "\n",
    "    assert not (self.condition_on_text_encodings and not exists(text_encodings)), 'text or text encodings must be passed into decoder if specified'\n",
    "    assert not (not self.condition_on_text_encodings and exists(text_encodings)), 'decoder specified not to be conditioned on text, yet it is presented'\n",
    "\n",
    "    assert not (exists(inpaint_image) ^ exists(inpaint_mask)), 'inpaint_image and inpaint_mask (boolean mask of [batch, height, width]) must be both given for inpainting'\n",
    "\n",
    "    img = None\n",
    "    if start_at_unet_number > 1:\n",
    "        # Then we are not generating the first image and one must have been passed in\n",
    "        assert exists(image), 'image must be passed in if starting at unet number > 1'\n",
    "        assert image.shape[0] == batch_size, 'image must have batch size of {} if starting at unet number > 1'.format(batch_size)\n",
    "        prev_unet_output_size = self.image_sizes[start_at_unet_number - 2]\n",
    "        img = resize_image_to(image, prev_unet_output_size, nearest = True)\n",
    "    is_cuda = next(self.parameters()).is_cuda\n",
    "    num_unets = self.num_unets\n",
    "    cond_scale = cast_tuple(cond_scale, num_unets)\n",
    "\n",
    "    for unet_number, unet, vae, channel, image_size, predict_x_start, learned_variance, noise_scheduler, lowres_cond, sample_timesteps, unet_cond_scale in tqdm(zip(range(1, num_unets + 1), self.unets, self.vaes, self.sample_channels, self.image_sizes, self.predict_x_start, self.learned_variance, self.noise_schedulers, self.lowres_conds, self.sample_timesteps, cond_scale)):\n",
    "        if unet_number < start_at_unet_number:\n",
    "            continue  # It's the easiest way to do it\n",
    "\n",
    "        context = one_unet_in_gpu(self, unet = unet, device=device) if is_cuda else null_context()\n",
    "        \n",
    "        with context:\n",
    "            # prepare low resolution conditioning for upsamplers\n",
    "\n",
    "            #lowres_cond_img = lowres_noise_level = None\n",
    "            lowres_noise_level = None\n",
    "            shape = (batch_size, channel, image_size, image_size)\n",
    "\n",
    "            is_latent_diffusion = isinstance(vae, VQGanVAE)\n",
    "            image_size = vae.get_encoded_fmap_size(image_size)\n",
    "            shape = (batch_size, vae.encoded_dim, image_size, image_size)\n",
    "            \n",
    "            lowres_cond_img = maybe(vae.encode)(lowres_cond_img)\n",
    "            \n",
    "            # denoising loop for image\n",
    "\n",
    "            img, psnrs = p_sample_loop_custom(\n",
    "                self=inference.model_manager.decoder_info.model,\n",
    "                unet=unet,\n",
    "                shape=shape,\n",
    "                img_start=image,\n",
    "                image_embed = image_embed,\n",
    "                text_encodings = text_encodings,\n",
    "                cond_scale = unet_cond_scale,\n",
    "                predict_x_start = predict_x_start,\n",
    "                learned_variance = learned_variance,\n",
    "                clip_denoised = not is_latent_diffusion,\n",
    "                lowres_cond_img = lowres_cond_img,\n",
    "                lowres_noise_level = lowres_noise_level,\n",
    "                is_latent_diffusion = is_latent_diffusion,\n",
    "                noise_scheduler = noise_scheduler,\n",
    "                timesteps = sample_timesteps,\n",
    "                inpaint_image = inpaint_image,\n",
    "                inpaint_mask = inpaint_mask,\n",
    "                inpaint_resample_times = inpaint_resample_times,\n",
    "                gt=gt\n",
    "            )\n",
    "\n",
    "            img = vae.decode(img)\n",
    "\n",
    "        if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n",
    "            break\n",
    "\n",
    "    return img, psnrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a3cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(pred, gt):\n",
    "    # pred, gt \\in [0, 1]\n",
    "    pred_int = (pred * 255).to(torch.uint8).cpu().numpy()\n",
    "    gt_int = (gt * 255).to(torch.uint8).cpu().numpy()\n",
    "    return 20 * np.log10(255) - 10 * np.log10(((pred_int - gt_int) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c1cc3",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd97a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CocoDataset(\n",
    "    im_dir='./data/images',\n",
    "    caption_dir='./data/captions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad67b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e822c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelLoadConfig.from_json_path('./configs/dalle2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65182d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: This decoder was trained on version 1.1.0 but the current version is 1.2.1. This may result in the model failing to load.\n",
      "FIX: Switch to this version with `pip install DALLE2-pytorch==1.1.0`. If different models suggest different versions, you may just need to choose one.\n",
      "WARNING: This decoder was trained on an old version of Dalle2. This may result in the model failing to load or it may lead to producing garbage results.\n",
      "WARNING: This prior was trained on an old version of Dalle2. This may result in the model failing to load or it may produce garbage results.\n"
     ]
    }
   ],
   "source": [
    "model_manager = DalleModelManager(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045b08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = ExampleInference(model_manager)\n",
    "inference.model_manager.decoder_info.model.sample_timesteps = (None, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8ea816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiinir/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 47.79it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.23it/s]\n",
      "2it [00:53, 26.53s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.94it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.14it/s]\n",
      "2it [00:53, 26.66s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.46it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.05it/s]\n",
      "2it [00:53, 26.79s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 49.18it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.96it/s]\n",
      "2it [00:53, 26.90s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 49.50it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.94it/s]\n",
      "2it [00:53, 26.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image = 1.jpg\n",
      "32.65313127129459\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 51.01it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.94it/s]\n",
      "2it [00:53, 26.93s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.23it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.93it/s]\n",
      "2it [00:53, 26.95s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.66it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.92it/s]\n",
      "2it [00:53, 26.97s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.97it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.92it/s]\n",
      "2it [00:53, 26.96s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.58it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.93it/s]\n",
      "2it [00:53, 26.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image = 2.jpg\n",
      "32.08402842591964\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 51.12it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.94it/s]\n",
      "2it [00:53, 26.94s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 48.91it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.95it/s]\n",
      "2it [00:53, 26.92s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.91it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.96it/s]\n",
      "2it [00:53, 26.92s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.98it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.97it/s]\n",
      "2it [00:53, 26.90s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 49.05it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.96it/s]\n",
      "2it [00:53, 26.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image = 3.jpg\n",
      "31.590432737255895\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.41it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.94it/s]\n",
      "2it [00:53, 26.93s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.40it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.93it/s]\n",
      "2it [00:53, 26.95s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.72it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.91it/s]\n",
      "2it [00:53, 26.97s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 49.98it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 18.94it/s]\n",
      "2it [00:53, 26.94s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.71it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.07it/s]\n",
      "2it [00:53, 26.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image = 4.jpg\n",
      "36.71517332465518\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.75it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.06it/s]\n",
      "2it [00:53, 26.76s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.40it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.06it/s]\n",
      "2it [00:53, 26.77s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.07it/s]\n",
      "2it [00:53, 26.76s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.76it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.09it/s]\n",
      "2it [00:53, 26.73s/it]\n",
      "sampling loop time step: 100%|██████████| 64/64 [00:01<00:00, 50.85it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:52<00:00, 19.11it/s]\n",
      "2it [00:53, 26.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image = 5.jpg\n",
      "30.37966538876232\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "psnrs = []\n",
    "\n",
    "interpolation_mode = 'bicubic'\n",
    "\n",
    "psnrs_from_steps = []\n",
    "for source_image_i in range(len(dataset)):\n",
    "    cur_image_psnrs_from_steps = []\n",
    "\n",
    "    source_image_name, source_image, source_captions = dataset[source_image_i]\n",
    "    max_psnr = 0\n",
    "    for text_str_i, text_str in enumerate(source_captions):\n",
    "        text = [text_str]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding_map = inference._sample_prior(text)\n",
    "            image_embedding = image_embedding_map[0][0].unsqueeze(0)\n",
    "            source_image_lowres_small = F.interpolate(source_image.unsqueeze(0), size=(64, 64), mode=interpolation_mode)\n",
    "        lowres_cond_img = F.interpolate(source_image_lowres_small, size=(256, 256), mode=interpolation_mode)\n",
    "        \n",
    "        inference.model_manager.decoder_info.model.to(device)\n",
    "        \n",
    "        lowres_cond_img = lowres_cond_img.to(device)\n",
    "        with torch.no_grad():\n",
    "            text_encodings = inference._encode_text(text).to(device)\n",
    "        image_embed = image_embedding.to(device)\n",
    "        source_image_lowres_small = source_image_lowres_small.to(device)\n",
    "        source_image = source_image.to(device)\n",
    "\n",
    "        source_image_lowres_small = source_image_lowres_small * 2. - 1.\n",
    "\n",
    "        source_image.requires_grad = False\n",
    "\n",
    "        res, cur_img_text_psnrs_from_steps = sample_custom(\n",
    "            self=inference.model_manager.decoder_info.model,\n",
    "            lowres_cond_img=lowres_cond_img,\n",
    "            image_embed = image_embed,\n",
    "            text_encodings = text_encodings,\n",
    "            image=source_image_lowres_small,\n",
    "            start_at_unet_number = 2,\n",
    "            gt=source_image\n",
    "        )\n",
    "\n",
    "        res = res.detach().squeeze(0)\n",
    "        cur_image_psnrs_from_steps.append(cur_img_text_psnrs_from_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            max_psnr = max(psnr(res, source_image).item(), max_psnr)\n",
    "    \n",
    "    psnrs_from_steps.append(cur_image_psnrs_from_steps)\n",
    "\n",
    "    print(f\"image = {source_image_name}\")\n",
    "    print(max_psnr)\n",
    "    print(\"\\n---------------------------\\n\")\n",
    "    psnrs.append(max_psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2595da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.68448622957752\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(psnrs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
